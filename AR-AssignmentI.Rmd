---
title: "AR - Assignment I"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## A

```{r}
library(ggplot2)
library(tidyverse)

rm(list=ls())

# Define x
x = seq(0,1, length.out=100)

#x = rnorm(100,0,1)
# We choose equally spaced notes over the range [0,1]

knots = seq(0,1,length.out=8) # we set this to 8 as the first and last knots will be removed, so this ensures we are left with 6

# Basis functions per Woods formulation

b1 = function(x) {
  rep(1, length(x))
}

b2 = function(x){
  x
}

R_x_z = function(x, z) {
  ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12) / 4 - 
  ((abs(x - z) - 0.5)^4 - 0.5 * (abs(x - z) - 0.5)^2 + 7/240) / 24
}

# Construct the basis matrix

basis_matrix = cbind(b1(x), b2(x),sapply(knots[-c(1,length(knots))],function(k) R_x_z(x,k)))

basis_design_matrix = basis_matrix %>% as_tibble()
                  
colnames(basis_design_matrix) = c("b1", "b2", "b3", "b4", paste0("b", 5:(4 + length(knots))))
basis_design_matrix = basis_design_matrix %>%
                       mutate(x = x)

basis_melted <- reshape2::melt(basis_design_matrix, id.vars = "x")

ggplot(basis_melted, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 1) + 
  labs(title = "Wood Cubic Spline Basis",
       x = "x", y = "Basis Function Value", color = "Basis") +
  #xlim(0, 1) + 
  theme_minimal()

```


# B 

```{r}
set.seed(123)
n <- 100
x = rnorm(n,0,1)

y <- 5 + sin(3 * pi * (x-0.6)) + rnorm(n, sd = 0.5^2)


knots <- seq(min(x), max(x), length.out = 32) # We define length of 32 because the first and last knots will be removed, to ensure we have exactly 30 knots

# Construct penalty matrix
knot_positions <- knots[-c(1, length(knots))]
n_knots = length(knot_positions)
S = matrix(0, n_knots, n_knots)
for(i in 1:n_knots){
  for (j in 1: n_knots){
    S[i,j] = R_x_z(knot_positions[i], knot_positions[j])
  }
}

penalized_regression_spline = function(lambda, S, knot_positions, y){
  basis_matrix <- cbind(
  b1(x),
  b2(x),
  sapply(knot_positions, function(k) R_x_z(x, k))
)
  P = lambda * S
  X  = basis_matrix[, 3:ncol(basis_matrix)]
  XtX_plus_p = t(X)%*%X + P 
  XtY = t(X)%*%y
  beta_hat = solve(XtX_plus_p)%*%XtY
  fitted_values = X %*% beta_hat
  H = X %*% solve(XtX_plus_p) %*% t(X)
  se_errors = sqrt(diag(H))
  return (list(fitted_values = fitted_values , se_errors = se_errors))
}

fit_result = penalized_regression_spline(0.00000001, S, knot_positions,y) # choose a random lambda value

plot_data <- data.frame(x = x, y = y, y_hat = fit_result$fitted_values, se_errors=fit_result$se_errors)

ggplot(plot_data, aes(x = x, y = y)) + 
  geom_point(color = 'black', alpha = 0.6) +
  geom_line(aes(y = y_hat), color = 'blue', size = 1.2) +
  labs(title = "Penalized Regression Spline Fit",
       x = "X",
       y = "Y") +
  theme_minimal()
```


## C

```{r}

# (95% confidence intervals)
upper <- fit_result$fitted_values + 1.96 * fit_result$se_errors
lower <- fit_result$fitted_values - 1.96 * fit_result$se_errors


plot_data <- data.frame(x = x, y = y, y_hat = fit_result$fitted_values, lower = lower, upper = upper)

ggplot(plot_data, aes(x = x, y = y)) + 
  geom_point(color = 'black', alpha = 0.6) +
  geom_line(aes(y = y_hat), color = 'blue', size = 1.2) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1, fill = 'blue') +
  labs(title = "Penalized Regression Spline Fit with Confidence Bands",
       x = "X",
       y = "Y") +
  theme_minimal()
```





# D

```{r}

compute_gcv = function(lambda){
  basis_matrix <- cbind(
  b1(x),
  b2(x),
  sapply(knot_positions, function(k) R_x_z(x, k))
)
  P = lambda * S
  X  = basis_matrix[, 3:ncol(basis_matrix)]
  XtX_plus_p = t(X)%*%X + P 
  XtY = t(X)%*%y
  beta_hat = solve(XtX_plus_p)%*%XtY
  fitted_values = X %*% beta_hat
  
  # Compute residuals
  residuals = y - fitted_values
  H = X %*% solve(XtX_plus_p)%*%t(X)
  edf = sum(diag(H))
  
  # Compute the GCV score
  n = length(y)
  gcv = sum(residuals^2)/(n*(1-edf/n)^2)
  return (gcv)
}

lambda_values <- seq(0.00000001, 1, length.out = 5000)
gcv_scores <- sapply(lambda_values, compute_gcv)

gcv_data <- data.frame(lambda = lambda_values, gcv = gcv_scores)

ggplot(gcv_data, aes(x = lambda, y = gcv)) +
  geom_line(color = 'blue', size = 1.2) +
  labs(title = "GCV as a Function of the Smoothing Parameter",
       x = "Smoothing Parameter (lambda)",
       y = "GCV Score") +
  theme_minimal()

```


# Question 2

```{r}


library(gamair)
library(tidyverse)
library(dplyr)

rm(list=ls())

# Basis functions per Woods formulation

b1 = function(x) {
  rep(1, length(x))
}

b2 = function(x){
  x
}

R_x_z = function(x, z) {
  ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12) / 4 - 
  ((abs(x - z) - 0.5)^4 - 0.5 * (abs(x - z) - 0.5)^2 + 7/240) / 24
}

data("brain")

brain_data = brain%>%as_tibble()%>%
             select(X,Y, medFPQ)

tensor_product_spline = function(x,y, response,x_knot_positions, y_knot_positions, x_penalty_matrix, y_penalty_matrix, lambda, penalize=FALSE) {
  
  # Scale x and y
  x_scaled = scale(x)
  y_scaled = scale(y)
  
  y_data = as.matrix(response)
  x_basis_matrix <- cbind(sapply(x_knot_positions, function(k) R_x_z(x_scaled, k)))
  y_basis_matrix <- cbind(sapply(y_knot_positions, function(k) R_x_z(y_scaled, k)))
  
  tensor_basis_matrix = compute_tensor_product(x_basis_matrix, y_basis_matrix)
  tensor_penalty_matrix = compute_tensor_product(x_penalty_matrix, y_penalty_matrix)
  
  P = lambda * tensor_penalty_matrix
  X  = tensor_basis_matrix[1:nrow(y_data),]
  #X = scale(X, scale=TRUE, center=TRUE)
  #P = scale(P, scale=TRUE, center=TRUE)
  XtX_plus_p = t(X)%*%X + P 
  
  # Add a small constant to the diagonal for numerical stability
  #epsilon = 1e-8
  #XtX_plus_p = XtX_plus_p + diag(epsilon, nrow(XtX_plus_p))
  
  #print(dim(t(X)))
  #print(dim(y_data))
  XtY = t(X)%*%y_data
 #print(dim(XtX_plus_p))
  beta_hat = MASS::ginv(XtX_plus_p)%*%XtY
  fitted_values = X %*% beta_hat
  H = X %*% MASS::ginv(XtX_plus_p) %*% t(X)
  se_errors = sqrt(diag(H))
  return (list(fitted_values = fitted_values , se_errors = se_errors))
 #return(tensor_basis_matrix)
}

compute_tensor_product = function(A, B){
  m1 = ncol(A)
  m2 = ncol(B)
  G = matrix(NA, nrow=nrow(A)*nrow(B), ncol = m1 * m2 )
  ccol <- 1
  for (j in 1:m1) {
    for (k in 1:m2) {
      G[, ccol] <- A[, j] * B[, k]
      ccol <- ccol + 1
    }
  }
  return (G)
}

x_knots <- seq(min(brain_data$X), max(brain$X), length.out = 10) 
y_knots <- seq(min(brain_data$Y), max(brain$Y), length.out = 10) 
#x_knot_positions <- x_knots[-c(1, length(x_knots))]
#y_knot_positions <- y_knots[-c(1, length(y_knots))]



# # Construct penalty matrix

# Construct penalty matrix
construct_penalty_matrix <- function(knots) {
  n_knots <- length(knots)
  S <- matrix(0, n_knots, n_knots)
  for (i in 1:n_knots) {
    for (j in 1:n_knots) {
      S[i, j] <- R_x_z(knots[i], knots[j])
    }
  }
  S
}
S_x = construct_penalty_matrix(x_knots)
S_y = construct_penalty_matrix(y_knots)

# n_x_knots = length(x_knot_positions)
# S_x = matrix(0, n_x_knots, n_x_knots)
#   for(i in 1:n_x_knots){
#    for (j in 1: n_x_knots){
#      S_x[i,j] = R_x_z(x_knot_positions[i], x_knot_positions[j])
#    }
#   }
# 
# n_y_knots = length(y_knot_positions)
# S_y = matrix(0, n_y_knots, n_y_knots)
#   for(i in 1:n_y_knots){
#    for (j in 1: n_y_knots){
#      S_y[i,j] = R_x_z(y_knot_positions[i], y_knot_positions[j])
#    }
#  }

tensor_product_spline_result = tensor_product_spline(
  brain_data$X, 
  brain_data$Y, 
  brain_data$medFPQ,
  x_knots, 
  y_knots,
  S_x,
  S_y,
  0.00001
  )


# Visualize the results
brain_data <- brain_data %>% mutate(Fitted = tensor_product_spline_result$fitted_values)

ggplot(brain_data, aes(x = X, y = Y)) +
  geom_tile(aes(fill = Fitted)) +
  geom_point(aes(color = medFPQ), size = 2, alpha = 0.6) +
  scale_fill_viridis_c() +
  labs(title = "Tensor Product Spline Fit",
       x = "X",
       y = "Y",
       fill = "Fitted medFPQ",
       color = "Observed medFPQ") +
  theme_minimal()

```
```{r}
# Create a data frame with original data and fitted values
plot_data <- data.frame(
  X = brain_data$X,
  Y = brain_data$Y,
  Original = brain_data$medFPQ,
  Fitted = tensor_product_spline_result$fitted_values
)

# Create a long format of the data for easier plotting
plot_data_long <- tidyr::pivot_longer(plot_data, 
                                      cols = c(Original, Fitted),
                                      names_to = "Type",
                                      values_to = "Value")

# Create the plot
ggplot(plot_data_long, aes(x = X, y = Y, fill = Value)) +
  geom_tile() +
  facet_wrap(~ Type) +
  scale_fill_viridis_c() +
  labs(title = "Original vs Fitted Values",
       x = "X coordinate",
       y = "Y coordinate",
       fill = "medFPQ") +
  theme_minimal() +
  theme(aspect.ratio = 1)

```


```{r}
library(gamair)
library(tidyverse)
library(dplyr)

rm(list=ls())

data("brain")

brain_data = brain%>%as_tibble()%>%
             select(X,Y, medFPQ)

# Radial Basis function
tps_basis = function(r){
  ifelse(r>0, r^2 * log(r), 0)
}

create_design_matrix = function(data, knots){
  n = nrow(data)
  m = nrow(knots)
  # Create matrices of coordinates
  data_mat <- as.matrix(data[, 1:2])
  knots_mat <- as.matrix(knots[, 1:2])
  
  # Compute pairwise distances
  distances <- fields::rdist(data_mat, knots_mat)
  
  # Apply the thin-plate spline basis function to all distances at once
  B <- apply(distances, c(1,2), tps_basis)
  
  A = cbind(1, data)
  X = cbind(A, B)
  return (list (X=X, A=A, B=B))
}

thin_plate_spline = function(data, lambda, num_knots){
  n = nrow(data)
  knot_indices = seq(1, n, length.out = num_knots) 
  knots = data[knot_indices, 1:2]
  #knots = seq(1, n, length.out = num_knots) 
  
  # Create design matrix
  design_matrix = create_design_matrix(data[,1:2], knots)
  X = as.matrix(design_matrix$X)
  A = design_matrix$A
  B = design_matrix$B
  
  # Create penalty matrix
  m = ncol(B)
  P = rbind(
    cbind(matrix(0,3,3), matrix(0,3,m)),
    cbind(matrix(0,m,3), B[knot_indices,])
  )
  y = as.matrix(data[, 3])
  XtX_plus_p = t(X)%*%X + (lambda * P )
  XtY = t(X)%*%y
  beta_hat = solve(XtX_plus_p)%*%XtY
  fitted_values = X %*% beta_hat
  colnames(fitted_values) = "Fitted"
  H = X %*% solve(XtX_plus_p) %*% t(X)
  se_errors = sqrt(diag(H))
  return (list(fitted_values = fitted_values , se_errors = se_errors))
}

tps_result = thin_plate_spline(brain_data, 0.0001, 10)

head(tps_result$fitted_values)

plot_data <- data.frame(
  X = brain_data$X,
  Y = brain_data$Y,
  Original = brain_data$medFPQ,
  Fitted = tps_result$fitted_values
)

# Reshape the data for plotting
plot_data_long <- tidyr::pivot_longer(plot_data, cols = c(Original, Fitted),names_to = "Type", values_to = "medFPQ")

p = ggplot(plot_data_long, aes(x = X, y = Y, fill = medFPQ)) +
  geom_tile() +
  facet_wrap(~ Type) +
  scale_fill_viridis_c() +
  labs(title = "Thin-Plate Spline: Original vs Fitted Values",
       x = "X", 
       y = "Y", 
       fill = "medFPQ") +
  theme_minimal() +
  theme(aspect.ratio = 1)

print(p)
ggsave("tps_original_vs_fitted.png", p, width = 12, height = 6)


```

# Question 3

```{r}
library(mgcv)
rm(list = ls())
pulsar_data = read.csv2("Pulsar.csv", header = TRUE, sep=",")
head(pulsar_data)

# Split the data into training and test sets (80% training, 20% test)
set.seed(10032024)
sample <- sample.int(n = nrow(pulsar_data), size = floor(.8*nrow(pulsar_data)), replace = F)
train <- pulsar_data[sample, ]
test <- pulsar_data[-sample, ]

# Separate features and target variable
X_train <- train[, !names(train) %in% "Class"]
y_train <- train$Class
X_test <- test[, !names(test) %in% "Class"]
y_test <- test$Class

# Fit Logistic Regession Model (GLM)

log_regression_model = glm(Class ~., data = train, family = binomial)


#gam_model <- gam(Class ~ s(Mean_Integrated) + s(SD) + s(EK) + s(Skewness) + s(Mean_DMSNR_Curve) + s(SD_DMSNR_Curve) + #s(EK_DMSNR_Curve) + s(Skewness_DMSNR_Curve), data = training_data, family = binomial)

```



```{r}
# Install and load the necessary package
library(earth)


# Fit the MARS model
mars_model <- earth(Class ~ ., data = train)


# Make predictions on the test data
y_pred <- predict(mars_model, newdata = X_test, type = "response")

# Convert predictions to binary outcomes
y_pred_binary <- ifelse(y_pred > 0.5, 1, 0)

# Evaluate the model
accuracy <- mean(y_pred_binary == y_test)
cat("Accuracy:", accuracy, "\n")

# Confusion matrix and classification report
table(Predicted = y_pred_binary, Actual = y_test)


```


```{r}

library(fds)
library(wavelets)
library(glmnet)
library(ggplot2)
#library(SCBmeanfd)
rm(list = ls())
# Load the phoneme data from fds package
data(aa)
data(ao)


plot(aa)
plot(ao)

```

